{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d81776-947d-4465-96e4-b72dbc06a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install huggingface_hub\n",
    "!huggingface-cli login \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759db30-ed44-4e6c-b914-0a018a2eef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15991c84-d3be-4554-9ba5-e374881b0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"abc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7562c9a-f0f1-4818-8073-ffa0639b395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# api = HfApi()\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"best.pt\",  # your model weights\n",
    "#     path_in_repo=\"pytorch_model.bin\",  # standard filename\n",
    "#     repo_id=\"your-username/earthguard-fastvit-classifier\",\n",
    "#     token=\"abc\"  # or rely on saved config\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c285f7bb-9482-4cb7-b28b-5eca813855cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 1\n",
      "Current device: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "# \ud83e\uddea Codex Sandbox: EarthGuard Vision Model Refactor\n",
    "# Project Goal: Improve inference speed, accuracy, and architectural robustness for object detection\n",
    "# Context: Moving beyond YOLO if necessary. Model should be optimized for real-world litter detection from street/vehicle-mounted cameras.\n",
    "\n",
    "# \u2705 Setup\n",
    "import torch\n",
    "import torchvision\n",
    "import timm  # PyTorch Image Models - for FastViT, ConvNeXt, etc.\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd021f04-f682-4efb-9022-77bb2c2c5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 1\n",
      "current device: NVIDIA GeForce RTX 3080 Ti\n",
      "Output shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# \ud83e\uddea Codex Sandbox: EarthGuard Vision Model Refactor\n",
    "# Project Goal: Improve inference speed, accuracy, and architectural robustness for object detection\n",
    "# Context: Moving beyond YOLO if necessary. Model should be optimized for real-world litter detection from street/vehicle-mounted cameras.\n",
    "\n",
    "# \u2705 Setup\n",
    "import torch\n",
    "import torchvision\n",
    "import timm  # PyTorch Image Models - for FastViT, ConvNeXt, etc.\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# \ud83e\udde0 Debugging Check\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "print(\"current device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# \u2705 Load image safely with existence check\n",
    "def load_sample_image(path):\n",
    "    try:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image file not found: {path}\")\n",
    "        return None\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# \ud83d\ude80 Load ConvNeXt model\n",
    "model = timm.create_model(\"convnext_tiny.in12k_ft_in1k\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# \ud83d\udd01 Run inference\n",
    "@torch.no_grad()\n",
    "def run_inference(model, image_tensor):\n",
    "    if image_tensor is None:\n",
    "        return None\n",
    "    return model(image_tensor)\n",
    "\n",
    "# \ud83d\udd02 Load and infer\n",
    "image_path = \"litter_survey_dataset/SampleImageToAnnotate/3B9D1465-EB6A-49E9-9B79-D3A19AB3B570_1_105_c.jpeg\"\n",
    "image_tensor = load_sample_image(image_path)\n",
    "output = run_inference(model, image_tensor)\n",
    "if output is not None:\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "# \ud83d\udce4 Upload model to Hugging Face (optional step)\n",
    "def upload_to_huggingface():\n",
    "    api = HfApi()\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"best.pt\",\n",
    "        path_in_repo=\"pytorch_model.bin\",\n",
    "        repo_id=\"yanglinfang/earthguard-fastvit-classifier\"\n",
    "    )\n",
    "    print(\"Model uploaded to Hugging Face.\")\n",
    "\n",
    "# \ud83d\udee0\ufe0f Future directions:\n",
    "# - Plug into Detectron2 or MMDetection for box prediction\n",
    "# - Add custom classification head for 'litter vs non-litter'\n",
    "# - Export to ONNX or TorchScript for mobile/embedded inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c650f7be-8015-466c-9630-48a802904fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"convnext_tiny.in12k_ft_in1k\", pretrained=True)\n",
    "# Alternatives known to work\n",
    "# \"efficientnet_b0\"\n",
    "# \"resnet50\"\n",
    "# \"vit_base_patch16_224\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf3ed6-6e49-4e1b-b21f-72dadc039651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Step: Train a Baseline YOLOv8 Detector\n\nWe'll start from a small pre-trained checkpoint and fine-tune on our custom dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to dataset YAML\n",
    "data_yaml = Path('train_dataset/train_dataset.yaml')\n",
    "\n",
    "# Update this to the folder containing your dataset on Windows\n",
    "dataset_root = Path(r'C:\\path\\to\\train_dataset')  # change as needed\n",
    "\n",
    "with open(data_yaml) as f:\n",
    "    data_dict = yaml.safe_load(f)\n",
    "data_dict['path'] = str(dataset_root)\n",
    "\n",
    "# Initialize YOLOv8 with a small model. The weight file should exist locally or in the cache.\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Fine-tune on our dataset for a few epochs\n",
    "model.train(data=data_dict, epochs=5, imgsz=640)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "metrics = model.val()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Windows Setup Notes\n\nIf you're running this notebook on Windows, set `dataset_root` in the previous cell to the path of your local dataset folder. The folder should contain `images/` and `labels/` subdirectories."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Optional: Export to ONNX for Deployment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model to ONNX format for use in other runtimes\n",
    "model.export(format='onnx', dynamic=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
